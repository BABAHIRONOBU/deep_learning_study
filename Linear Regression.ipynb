{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Linear Regression"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Hypothesis\n$$H(x) = W(x) + b$$  \n\n$x$: training data, actual data  \n$W$: Weight  \n$b$: bias"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cost\n$$cost(W) = \\frac{1}{m} \\sum_{i=1}^m (Wx_i - y_i)^2$$\n$$cost(W, b) = \\frac{1}{m} \\sum_{i=1}^m (H(x_i) - y_i)^2$$  \n\n$Wx_i$, $H(x_i)$: expected value  \n$y_i$: actual value  \n$(H(x_i) - y_i)^2$: loss, error  \n$\\frac{1}{m} \\sum_{i=1}^m ()$: mean value of loss"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Goal"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "find $W$ and $b$ which minimize $cost(W,b)$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## How to find W and b: Gradient Descent Algorithm "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$W := W - \\alpha \\frac{\\partial cost(W,b)}{\\partial W}$$\n$$b := b - \\alpha \\frac{\\partial cost(W,b)}{\\partial b}$$  \n\n$\\alpha$: learning rate  \n$\\frac{\\partial z}{\\partial x}$: gradient"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exercise1"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\ntf.enable_eager_execution()  # important",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### $H(x) = Wx + b$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_data = [1., 2., 3., 4., 5.]\ny_data = [1., 2., 3., 4., 5.]\n\nW = tf.Variable(2.9)  # random value\nb = tf.Variable(0.5)  # random value\n\n# hypothesis = W * x + b\nhypothesis = W * x_data + b",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### $cost(W, b) = \\frac{1}{m} \\sum_{i=1}^m (H(x_i) - y_i)^2$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cost = tf.reduce_mean(tf.square(hypothesis - y_data))",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* tf.reduce_mean()"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "v = [1., 2., 3., 4.]\ntf.reduce_mean(v)  # 2.5",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2.5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* tf.square()"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tem = tf.square(3)  # 9",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": "9\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### minimize $cost(W,b)$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# learning_rate initialize\nlearning_rate = 0.01\n\n# Gradient descent\nfor i in range(100):    \n    with tf.GradientTape() as tape:\n        hypothesis = W * x_data + b\n        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n        \n    W_grad, b_grad = tape.gradient(cost, [W, b])\n    \n    W.assign_sub(learning_rate * W_grad)  # W = W - learning_rate*dcost_dW\n    b.assign_sub(learning_rate * b_grad)  # b = b - learning_rate*dcost_db\n    \n    if i%10 == 0:\n        print(\"{:5}|{:10.4}|{:10.4}|{:10.6}\".format(i, W.numpy(), b.numpy(), cost.numpy()))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "    0|     2.452|     0.376|     45.66\n   10|     1.104|  0.003398|  0.206336\n   20|     1.013|  -0.02091|0.00102611\n   30|     1.007|  -0.02184|9.26298e-05\n   40|     1.006|  -0.02123|8.26522e-05\n   50|     1.006|  -0.02053|7.72211e-05\n   60|     1.005|  -0.01984|7.2163e-05\n   70|     1.005|  -0.01918|6.74368e-05\n   80|     1.005|  -0.01854|6.30191e-05\n   90|     1.005|  -0.01793|5.88925e-05\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "* tf.GradientTape()"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = tf.constant(3.)\n\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = x * x\n\ndy_dx = tape.gradient(y, x)  # 6.0",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tf.Tensor(6.0, shape=(), dtype=float32)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "z = tf.Variable(3.0)\n\nwith tf.GradientTape() as tape:\n    y = z * z  # z is automatically watched since tf.Variable is trainable variable\n    \ndy_dz = tape.gradient(y, z)  # 6.0",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "6.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = tf.constant(2.)\nd = tf.constant(5.)\n\nwith tf.GradientTape() as tape:\n    tape.watch([a, d])\n    c = a*a + d*d\n    \ndc_da, dc_dd = tape.gradient(c, [a, d])  # 4, 10",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predict"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(W * 5. + b)  # expected value: 5\nprint(W * 2.5 + b)  # expected value: 2.5",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tf.Tensor(5.001219, shape=(), dtype=float32)\ntf.Tensor(2.4990265, shape=(), dtype=float32)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exercise2"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\ntf.enable_eager_execution()\nimport numpy as np",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### numpy training"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vector_form = np.array([1, 2])\nprint(vector_form.T)\nprint(vector_form == vector_form.T)\n\narray_form = np.array([[1, 2]])\nprint(array_form.T)\nprint(array_form == array_form.T)",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[1 2]\n[ True  True]\n[[1]\n [2]]\n[[ True False]\n [False  True]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "array = np.array([[1, 2], [3, 4]])\nvector = np.array([1, 1])\narr = np.array([[1, 1]])\n\n# *: dot product\n# array * vector = vector * array = vector\nprint(array.dot(vector))\nprint(vector.dot(array))\nprint(vector.dot(arr.T))\n\n# array * array = array\nprint(array.dot(arr.T))",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[3 7]\n[3 7]\n[4 6]\n[2]\n[[3]\n [7]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vector_t = np.array([1, 2, 3])\narray_t = np.array([[0, 1], [2, 3]])\nnum = np.array([1])  # or 1\n\nprint(vector_t + num)\nprint(array_t + num)\nprint(tf.square(vector_t))",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[2 3 4]\n[[1 2]\n [3 4]]\ntf.Tensor([1 4 9], shape=(3,), dtype=int64)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list = [1, 2, 3]\nlist2 = [list, list]\nprint(list2)\n\nl = list2[0]\nprint(l)\n\narray = np.array([1, 2, 3])\narray2 = np.array([array, array])\nprint(array2)\nprint(array is array2[0])\nprint()\n\nar = array2[:, [-1]]\nprint(ar)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[1, 2, 3], [1, 2, 3]]\n[1, 2, 3]\n[[1 2 3]\n [1 2 3]]\nFalse\n[[3]\n [3]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t_data = np.array([[1., 1., 1.], [2., 2., 2.]], dtype=np.float32)\nprint(t_data)\n\nt = tf.Variable(tf.random_normal([3, 1]))  # t is trainable variable\nt2 = tf.random_normal([3, 1])  # t2 is not trainable variable\nprint(t.numpy())\nprint(t2.numpy())\n\nt3 = tf.Variable(np.array([[3, 2, 1], [1, 2, 3]]))\nprint(t3.numpy())\n\nprint(tf.matmul(t_data, t))  # arguments of tf.matmul are must be matrix(not vector)",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[1. 1. 1.]\n [2. 2. 2.]]\n[[ 0.40577587]\n [-1.0215772 ]\n [-1.0294142 ]]\n[[ 0.17296557]\n [-0.1539809 ]\n [ 1.1171927 ]]\n[[3 2 1]\n [1 2 3]]\ntf.Tensor(\n[[-1.6452155]\n [-3.290431 ]], shape=(2, 1), dtype=float32)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### $H(X) = XW + b$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# data and label\nX1 = np.array([73., 80., 75.])\nX2 = np.array([93., 88., 93.])\nX3 = np.array([89., 91., 90.])\nX4 = np.array([96., 98., 100.])\nX5 = np.array([73., 66., 70.])\nX_data = np.array([X1, X2, X3, X4, X5], dtype=np.float32)\n# data type: int, double, float...\n\n\ny1 = np.array([152.])\ny2 = np.array([185.])\ny3 = np.array([180.])\ny4 = np.array([196.])\ny5 = np.array([142.])\nY_data = np.array([y1, y2, y3, y4, y5])\n\n# # weights and bias\n# w1 = tf.Variable(10.)\n# w2 = tf.Variable(10.)\n# w3 = tf.Variable(10.)\n# b_2 = tf.Variable(10.)\n# W_2 = np.array([w1, w2, w3])\n\nW_2 = tf.Variable(tf.random_normal([3, 1]))\nb_2 = tf.Variable(tf.random_normal([1]))\n\nhypothesis_2 = tf.matmul(X_data, W_2) + b_2",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### $cost(W,b) = \\frac{1}{m} \\sum_{i=1}^m (H(X_i) - Y_i)^2$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cost_2 = tf.reduce_mean(tf.square(hypothesis_2 - Y_data))",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### minimize $cost(W,b)$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# learning_rate\nlearning_rate_2 = 0.000001\n\n# gradient descent\nfor i in range(1000):\n    with tf.GradientTape() as tape:\n        hypothesis_2 = tf.matmul(X_data, W_2) + b_2\n        cost_2 = tf.reduce_mean(tf.square(hypothesis_2 - Y_data))\n    \n    W_2_grad, b_2_grad = tape.gradient(cost_2, [W_2, b_2])\n    \n    W_2.assign_sub(learning_rate_2 * W_2_grad)\n    b_2.assign_sub(learning_rate_2 * b_2_grad)\n    \n    # results are changed whenever data is trained\n    if i%50 == 0:\n        print(\"{:5}|{:12.4f}\".format(i, cost_2.numpy()))",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": "    0|      0.7928\n   50|      0.7914\n  100|      0.7899\n  150|      0.7885\n  200|      0.7870\n  250|      0.7856\n  300|      0.7842\n  350|      0.7827\n  400|      0.7813\n  450|      0.7798\n  500|      0.7784\n  550|      0.7770\n  600|      0.7756\n  650|      0.7742\n  700|      0.7728\n  750|      0.7714\n  800|      0.7700\n  850|      0.7686\n  900|      0.7672\n  950|      0.7658\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predict"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_data = np.array([[73., 80., 75.], [77., 75., 90.], [96, 98, 100], [73, 66, 70], [10, 24, 15]], dtype=np.float32)\npredict_data = tf.matmul(test_data, W_2) + b_2\nprint(predict_data.numpy())",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[150.50476 ]\n [156.96814 ]\n [195.58366 ]\n [142.9451  ]\n [ 25.828838]]\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}