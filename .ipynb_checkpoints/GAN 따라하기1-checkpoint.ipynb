{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# GAN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Library setting"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib\n\nimport keras\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.models import Model, Sequential\nfrom keras.datasets import mnist\nfrom tqdm import tqdm\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import Adam",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Loading data from mnist"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n    \n    # convert shape of x_train from (60000, 28, 28) to (60000, 784)\n    # 784 columns per row\n    x_train = x_train.reshape(60000, 784)\n    return (x_train, y_train, x_test, y_test)\n\n(X_train, y_train, X_test, y_test) = load_data()\nprint(X_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Adam optimizer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def adam_optimizer():\n    return Adam(lr=0.0002, beta_1=0.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Generator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_generator():\n    generator=Sequential()\n    generator.add(Dense(units=256, input_dim=100))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=512))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=784, activation='tanh'))\n    \n    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n\n    return generator\n\ng = create_generator()\ng.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Discriminator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_discriminator():\n    discriminator = Sequential()\n    \n    discriminator.add(Dense(units=1024, input_dim=784))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    \n    discriminator.add(Dense(units=512))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    \n    discriminator.add(Dense(units=256))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(units=1, activation='sigmoid'))\n    \n    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    \n    return discriminator\n\nd = create_discriminator()\nd.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Combine the Generator and Discriminator"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_gan(discriminator, generator):\n    discriminator.trainable = False\n    gan_input = Input(shape=(100,))\n    \n    x = generator(gan_input)\n    gan_output = discriminator(x)\n    gan = Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return gan\n\ngan = create_gan(d, g)\ngan.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Plot function"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_generated_images(epoch, generator, examples=100, \n                          dim=(10, 10), figsize=(10, 10)):\n    noise = np.random.normal(loc=0, scale=1, size=[examples, 100])\n    generated_images = generator.predict(noise)\n    generated_images = generated_images.reshape(100, 28, 28)\n    plt.figure(figsize=figsize)\n    \n    for i in range(generated_images.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generated_images[i], interpolation='nearest')\n        plt.axis('off')\n        \n    plt.tight_layout()\n        \n    plt.savefig('gan_generated_image %d.png' %epoch)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def training(epochs=1, batch_size=128):    \n    \n    #Loading the data    \n    (X_train, y_train, X_test, y_test) = load_data()\n    batch_count = X_train.shape[0] / batch_size\n    \n    \n    # Creating GAN    \n    generator= create_generator()\n    discriminator= create_discriminator()\n    gan = create_gan(discriminator, generator)\n    \n    for e in range(1,epochs+1 ):\n        print(\"Epoch %d\" %e)\n        \n        for _ in tqdm(range(batch_size)):\n            # Generate random noise as an input to initialize the generator            \n            noise= np.random.normal(0,1, [batch_size, 100])\n                     \n            # Generate fake MNIST images from noised input            \n            generated_images = generator.predict(noise)\n            \n            # Get a random set of real images            \n            image_batch =X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n            \n            # Construct different batches of  real and fake data \n            X= np.concatenate([image_batch, generated_images])\n            \n            # Labels for generated and real data            \n            y_dis=np.zeros(2*batch_size)\n            y_dis[:batch_size]=0.9\n            \n            # Pre train discriminator on fake and real data before starting the gan. \n            discriminator.trainable=True\n            discriminator.train_on_batch(X, y_dis)\n                       \n            # Tricking the noised input of the Generator as real data            \n            noise= np.random.normal(0,1, [batch_size, 100])\n            y_gen = np.ones(batch_size)\n                      \n            # During the training of gan, \n            # the weights of discriminator should be fixed. \n            # We can enforce that by setting the trainable flag\n            discriminator.trainable=False\n            \n            # training  the GAN by alternating the training of the Discriminator \n            # and training the chained GAN model with Discriminatorâ€™s weights freezed.\n            gan.train_on_batch(noise, y_gen)\n            \n        if e == 1 or e % 20 == 0:\n            plot_generated_images(e, generator)\n                \ntraining(400, 128)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}